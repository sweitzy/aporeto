#!/usr/bin/env python3

# problem3: see https://github.com/aporeto-inc/quiz

'''
### Problem3: Go sample:

You will need to implement a program in go as described below:

gosample [-help|-h]
gosample -urls=<comma-seperated-one-or-more-urls>

This binary should be able to read multiple urls and generate a word (a word only contains letters A-Za-z0-9) frequency table. Output can be stored in files with names url1.txt, url2.txt â€¦ in the following format:
  url: <url>
    word1: count
    word2: count

Example:

See samples/problem3/url1.txt

**[Extra Credit]**

Handle URLs in parallel and demonstrate reduced time to process all URLs. Note: You only need to implement serial or parallel but not both.
'''

import sys
import getopt
import requests
import re

def usage(exit_code=2):
    print('pysample [--help|-h]')
    print('pysample --urls=<comma-seperated-one-or-more-urls>')
    sys.exit(exit_code)

def main():

    try:
        # NOTE: have to use --help/--urls, not -help/-urls.
        opts, args = getopt.getopt(sys.argv[1:], 'h', ['urls=', 'help'])
    except getopt.GetoptError as err:
        # print help information and exit:
        print(err)  # will print something like 'option -a not recognized'
        usage()

    urls = None

    # TODO: try argparse instead

    for o, a in opts:
        if o in ('-h', '--help'):
            usage(0)
        elif o == '--urls':
            urls = a 
        else:
            assert False, 'unhandled option'

    if len(args) > 0:
        print('Unwanted args', args)
        usage()

    if urls is None:
        print('Missing --urls')
        usage()

    # TODO: for some reason, https://www.aporeto.com returns 403, but curl works!
    url_counter = 1
    for url in urls.split(','):

        # NOTE: If we want to be REALLY efficient, use stream=True on request.
        resp = requests.get(url)
        if resp.status_code != requests.codes.ok:
            raise NameError('Did not get ok response code')

        # Process the response using iterator for efficiency.
        # NOTE: use \w to get more realistic value for words,
        # instead of [a-zA-Z0-9]
        # NOTE: If we want to be REALLY efficient, use resp.iter_lines() not resp.text.

        seen = dict()
        for match in re.finditer(r'\w+', resp.text):
            word = match.group(0)
            if not word in seen:
                seen[word] = 1
            else:
                seen[word] += 1

        with open('url' + str(url_counter) + '.txt', 'w') as f:
            f.write('url: {}\n'.format(url))
            for k, v in seen.items():
                f.write('    {}: {}\n'.format(k, v))

        url_counter += 1

if __name__ == '__main__':
    main()
